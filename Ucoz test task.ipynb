{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формулировка задачи:\n",
    "\n",
    "Необходимо создать консольную (работающую из командной строки) утилиту. Программа спрашивает домен и забирает с введённого адреса файл robots.txt. Затем скрипт парсит файл и выводит его содержимое в виде объекта. Ключами объекта являются параметры User-Agent, а значениями — вложенный объект. Вложенный объект содержит два поля Allow и Dissallow, каждый из которых является массивом соответствующих URL из robots.txt. Код выложить на GitHub\n",
    "Пример:\n",
    "\n",
    "{\n",
    "  \"*\": {\n",
    "      \"Disallow\": [\"/cgi-bin\"],\n",
    "      \"Allow\": [\"/\"]\n",
    "   },\n",
    "  \"GoogleBot\": {\n",
    "      \"Disallow\": [\"/cgi-bin\"],\n",
    "      \"Allow\": [\"/\"]\n",
    "   }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Декомпозиция задачи:\n",
    "1. У нас консольная программа, но передавать параметры при запуске вроде бы не нужно. Есть слово \"скрипт\", но нет ограничения на язык. Пусть будет python, его (при прочих равных) приятно читать и видеть. Системой пусть будет linux, но ничего платформоспецифичного вроде бы не предполагается.\n",
    "\n",
    "2. Нужно спросить о домене и забрать с введённого адреса. Предполагаю, что \"домен\" и \"адрес\" здесь понимаются как синонимы, а не как \"домен + адрес\" или как-то ещё. Есть два способа это сделать - написать текстом строку и вбить IP-адрес. Можно поставить проверки на основе регулярных выражений, которые покажут, является ли введённый текст чем-то осмысленным. Ещё можно ругаться, если нет такого адреса, но для этого вроде бы есть готовая ругалка\n",
    "\n",
    "3. Нужно вытащить файл и прочитать. Возможные проблемы: по адресу нет файла, нет нужной структуры внутри.\n",
    "\n",
    "4. Чтобы вернуть что нужно, можно было бы определить пользовательский объект. Но вид объекта наводит на мысль либо о словаре словарей, который уже реализован. Воспользуемся им.\n",
    "\n",
    "5. В требованиях есть слово \"выводит содержимое в виде объекта\". По некоторому размышлению, предполагает, что мы, возможно, хотим уметь сериализовать (или хотя бы выводить на печать) полученный объект, чтобы прямо в консоли видеть результат. С этим тоже нет проблемы, но для красивого вывода можно пробежаться по ключам, чтобы не смотреть совсем уж на мешанину.\n",
    "\n",
    "Методы:\n",
    "\n",
    "1. Получить url\n",
    "2. Соединиться\n",
    "3. Распарсить\n",
    "4. Прибраться за собой"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "Замечу, что на stackoverflow <a href = https://stackoverflow.com/questions/43085744/parsing-robots-txt-in-python#43086135> есть </a> наводящее на нужные мысли (правда, для python 2) решение такого вот вида:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Allowed': ['/wp-admin/admin-ajax.php'], 'Disallowed': ['/wp-admin/', '/search/', '/wp-login.php', '/activate/', '/cgi-bin/', '/mshots/v1/', '/next/', '/public.api/']}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "result = os.popen(\"curl https://fortune.com/robots.txt\").read()\n",
    "result_data_set = {\"Disallowed\":[], \"Allowed\":[]}\n",
    "\n",
    "for line in result.split(\"\\n\"):\n",
    "    if line.startswith('Allow'):    # this is for allowed url\n",
    "        result_data_set[\"Allowed\"].append(line.split(': ')[1].split(' ')[0])    # to neglect the comments or other junk info\n",
    "    elif line.startswith('Disallow'):    # this is for disallowed url\n",
    "        result_data_set[\"Disallowed\"].append(line.split(': ')[1].split(' ')[0])    # to neglect the comments or other junk info\n",
    "\n",
    "print (result_data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме того, оно не учитывает наличие нескольких user-agent.\n",
    "\n",
    "Дополним его и избавим от указанных проблем, попутно придав формат тестового задания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests, os, re\n",
    "# получим этот url от пользователя потом\n",
    "\n",
    "def is_string_a_valid_url(string):\n",
    "    # проверка корректности на случай, если это адрес, идея отсюда: \n",
    "    # https://web.izjum.com/regexp-email-url-phone\n",
    "    url_regexp = '^((https?|ftp)\\:\\/\\/)?([a-z0-9]{1})((\\.[a-z0-9-])|([a-z0-9-]))*\\.([a-z]{2,6})(\\/?)$'\n",
    "    if re.match(url_regexp, string) != None:\n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "\n",
    "def is_string_a_valid_ip(string):\n",
    "    # и на случай, если это IP-адрес.\n",
    "    # выражение - отсюда https://a-panov.ru/регулярные-выражения-проверка-ip-на-кор/\n",
    "    ip_regexp = '/^(25[0-5]|2[0-4][0-9]|[0-1][0-9]{2}|[0-9]{2}|[0-9])(\\.(25[0-5]|2[0-4][0-9]|[0-1][0-9]{2}|[0-9]{2}|[0-9])){3}$'\n",
    "    if re.match(ip_regexp, string) != None:\n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "\n",
    "def ask_for_url():\n",
    "    url = input(\"Enter the url you wich to retrieve 'robots.txt' from: \")\n",
    "    \n",
    "    # если это не цифры, и не начали с описания протокола, добавим по умолчанию https\n",
    "    if ((not url.startswith('http')) and (not url[0].isdigit())):\n",
    "        url = \"https://\" + url\n",
    "    \n",
    "    if (is_string_a_valid_url(url) == True) or (is_string_a_valid_ip(url) == True):\n",
    "        url += '/robots.txt'\n",
    "        return url\n",
    "    else:\n",
    "        print(\"You're probably entering not a valid url. Please make sure the input is correct.\")\n",
    "        return 1\n",
    "\n",
    "\n",
    "def download_robots_txt(url, my_user_agent = '*'):\n",
    "    # мы не знаем, какой/какие user-agent будем использовать, так что подадим его пока явно\n",
    "    local_filename = \"robots_local_temp.txt\"\n",
    "    my_headers = requests.utils.default_headers()\n",
    "    my_headers.update({'User-Agent': my_user_agent})\n",
    "    r = requests.get(url, headers = my_headers)\n",
    "    # будем качать по частям, на случай, если файл очень велик\n",
    "    with open(local_filename, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=1024): \n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "    logstring = 'download finished, temporary file ./' + local_filename + ' created' \n",
    "    print(logstring)\n",
    "    return 0\n",
    "\n",
    "# избавимся от файла, чтобы он не болтался в каталоге\n",
    "def dispose_of_temp_file():\n",
    "    if os.path.exists(\"robots_local_temp.txt\"):\n",
    "        os.remove(\"robots_local_temp.txt\")\n",
    "        print('cleaning up...')\n",
    "    return 0\n",
    "\n",
    "# распарсим имеющийся файл\n",
    "def parse_robots_txt():\n",
    "    if os.path.exists(\"robots_local_temp.txt\"):\n",
    "        local_file = open('robots_local_temp.txt', 'r')\n",
    "        resulting_data_set = {}\n",
    "        data_set_for_user_agent = {\"Disallowed\":[], \"Allowed\":[]}\n",
    "        agent_name_current = 'NoAgentYet'\n",
    "        for line in local_file:\n",
    "            line = line.split('\\n')[0]\n",
    "            if line.startswith('User-agent'):\n",
    "                agent_name_next = line.split(': ')[1].split(' ')[0]\n",
    "                # пока первого агента нет, будет пустое поле в словаре. Зато после изменения агента мы \n",
    "                # запишем все собранные сведения\n",
    "                # сотрём накопленное и продолжим наполнять словарь.\n",
    "                if not agent_name_current == 'NoAgentYet':\n",
    "                    resulting_data_set[agent_name_current] = data_set_for_current_user_agent\n",
    "                agent_name_current = agent_name_next\n",
    "                data_set_for_current_user_agent = {\"Disallowed\":[], \"Allowed\":[]}\n",
    "            else:\n",
    "                if line.startswith('Allow'):\n",
    "                    # комментарии и мусор немного отфильтруем\n",
    "                    data_set_for_current_user_agent[\"Allowed\"].append(line.split(': ')[1].split(' ')[0])    \n",
    "                elif line.startswith('Disallow'):\n",
    "                    # аналогично\n",
    "                    data_set_for_current_user_agent[\"Disallowed\"].append(line.split(': ')[1].split(' ')[0])\n",
    "        #после выхода из цикла мы останемся со сведениями о последнем (возможно, единственном) агенте\n",
    "        agent_name_current=agent_name_next\n",
    "        resulting_data_set[agent_name_current] = data_set_for_current_user_agent\n",
    "        return resulting_data_set\n",
    "    else:\n",
    "        print('A local copy of robots.txt is missing for some reason.')\n",
    "        return 1\n",
    "    \n",
    "def print_out_the_result():\n",
    "    result = parse_robots_txt()\n",
    "    if result == 1:\n",
    "        print(\"No output created\")\n",
    "        return 1\n",
    "    else:\n",
    "        for key in result.keys():\n",
    "            print(key, ':')\n",
    "            for inner_key in result[key]:\n",
    "                print(inner_key, ':')\n",
    "                print(result[key][inner_key])\n",
    "            print()\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning up...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dispose_of_temp_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the url you wich to retrieve 'robots.txt' from: habrahabr.ru\n",
      "https://habrahabr.ru/robots.txt\n"
     ]
    }
   ],
   "source": [
    "url = ask_for_url()\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download finished, temporary file ./robots_local_temp.txt created\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_robots_txt(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Yandex': {'Allowed': [], 'Disallowed': ['/search/', '/js/', '/css/', '/ajax/', '/login/', '/register/', '/*utm_']}, '*': {'Allowed': [], 'Disallowed': ['/search/', '/js/', '/css/', '/ajax/', '/login/', '/register/', '/*utm_']}, 'Slurp': {'Allowed': [], 'Disallowed': ['/search/', '/js/', '/css/', '/ajax/', '/login/', '/register/', '/*utm_']}, 'Googlebot': {'Allowed': [], 'Disallowed': ['/search/', '/js/', '/css/', '/ajax/', '/login/', '/register/', '/*utm_']}}\n"
     ]
    }
   ],
   "source": [
    "result = parse_robots_txt()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yandex :\n",
      "Allowed :\n",
      "[]\n",
      "Disallowed :\n",
      "['/search/', '/js/', '/css/', '/ajax/', '/login/', '/register/', '/*utm_']\n",
      "\n",
      "* :\n",
      "Allowed :\n",
      "[]\n",
      "Disallowed :\n",
      "['/search/', '/js/', '/css/', '/ajax/', '/login/', '/register/', '/*utm_']\n",
      "\n",
      "Slurp :\n",
      "Allowed :\n",
      "[]\n",
      "Disallowed :\n",
      "['/search/', '/js/', '/css/', '/ajax/', '/login/', '/register/', '/*utm_']\n",
      "\n",
      "Googlebot :\n",
      "Allowed :\n",
      "[]\n",
      "Disallowed :\n",
      "['/search/', '/js/', '/css/', '/ajax/', '/login/', '/register/', '/*utm_']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_out_the_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning up...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dispose_of_temp_file()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
